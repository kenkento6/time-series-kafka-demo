{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_global_config, get_waveform_path, get_base_time, get_ending_time, plot_waveform,build_spark_session\n",
    "\n",
    "from pyspark.sql.functions import datediff, to_date, max as max_, lit, col, collect_list, row_number, concat_ws, format_number, concat, monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.pandas.frame import DataFrame\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import wfdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 18:46:14 WARN Utils: Your hostname, Michaels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.118 instead (on interface en0)\n",
      "23/04/09 18:46:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/michaelscott/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/michaelscott/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/michaelscott/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-87436ffe-a77a-4dfb-a9c6-3624454ead13;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in spark-list\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 7017ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from spark-list in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   3   |   3   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-87436ffe-a77a-4dfb-a9c6-3624454ead13\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/17ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 18:46:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "cfg = get_global_config()\n",
    "spark = build_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_into_pyspark(file, schema=None, cache=False):\n",
    "    if not schema:\n",
    "        df=spark.read.csv(f'file:/{cfg[\"MIMICPATH\"]}/{file}.csv.gz', header=True, inferSchema=True)\n",
    "    else:\n",
    "        df=spark.read.csv(f'file:/{cfg[\"MIMICPATH\"]}/{file}.csv.gz', header=True, schema=schema)\n",
    "    if cache:\n",
    "        df.cache()\n",
    "    df.createOrReplaceTempView(file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "PATIENTS = read_into_pyspark('PATIENTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ROW_ID: int, SUBJECT_ID: int, GENDER: string, DOB: timestamp, DOD: timestamp, DOD_HOSP: timestamp, DOD_SSN: timestamp, EXPIRE_FLAG: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|SUBJECT_ID|\n",
      "+----------+\n",
      "|      2659|\n",
      "|      3794|\n",
      "|      4900|\n",
      "|      4935|\n",
      "|      7253|\n",
      "|     12799|\n",
      "|     13840|\n",
      "|     15619|\n",
      "|     15727|\n",
      "|     17753|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find matched patients\n",
    "# The file \"/Users/michaelscott/bd4h/project/data/waveform/physionet.org/files/mimic3wdb-matched/1.0/RECORDS\"\n",
    "# contains a list of all the records in the waveform database.\n",
    "# Each line is \"p00/p000085/\"\n",
    "# Now, extract all the patient IDs that are present in this file\n",
    "# and that will be used to filter all the other tables.\n",
    "\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "# Read the RECORDS file as a text file and split each line into patient IDs\n",
    "# convert string to int\n",
    "patient_ids = spark.read.text(f\"file://{cfg['WAVEFPATH']}/RECORDS\") \\\n",
    "                    .select(substring(\"value\", 6, 6).alias(\"SUBJECT_ID\")) \\\n",
    "                    .selectExpr(\"CAST(SUBJECT_ID AS INT) AS SUBJECT_ID\")\\\n",
    "                    .distinct()\n",
    "\n",
    "patient_ids.createOrReplaceTempView('matched_patients')\n",
    "# Show the first 10 patient IDs in the DataFrame\n",
    "patient_ids.show(10)\n",
    "\n",
    "# Use the patient IDs to filter another PySpark table\n",
    "# other_table_filtered = other_table.join(patient_ids, on=\"patient_id\", how=\"inner\")\n",
    "\n",
    "# Get all patient_id from matched_patients as a list\n",
    "patientid = patient_ids.select('SUBJECT_ID').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_age_df = spark.sql(\"\"\"SELECT matched_patients.SUBJECT_ID, date_format(DOB, 'yyyy-MM-dd') dob FROM patients \n",
    "INNER JOIN matched_patients \n",
    "ON matched_patients.subject_id = patients.subject_id\"\"\")\n",
    "\n",
    "# Write df to CSV\n",
    "patients_age_df.coalesce(1).write.csv(f'file://{cfg[\"EXPLOREPATH\"]}/patients_age', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "\n",
    "# Convert Pandas DataFrame to SQL insert statements\n",
    "insert_sql = pdf.to_sql(table_name, conn, if_exists=\"append\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd4hproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
