{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>PULSE</th>\n",
       "      <th>RESP</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>NBPSys</th>\n",
       "      <th>NBPDias</th>\n",
       "      <th>NBPMean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2112-05-23 14:34:23.221000000</th>\n",
       "      <td>73.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-23 14:35:23.220999999</th>\n",
       "      <td>72.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-23 14:36:23.220999998</th>\n",
       "      <td>76.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-23 14:37:23.220999997</th>\n",
       "      <td>74.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-23 14:38:23.220999996</th>\n",
       "      <td>75.3</td>\n",
       "      <td>41.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>94.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-24 17:34:23.220998380</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-24 17:35:23.220998379</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-24 17:36:23.220998378</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-24 17:37:23.220998377</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112-05-24 17:38:23.220998376</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1625 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 HR  PULSE  RESP  SpO2  NBPSys  NBPDias  \\\n",
       "2112-05-23 14:34:23.221000000  73.6    0.0   7.9   0.0     NaN      NaN   \n",
       "2112-05-23 14:35:23.220999999  72.3    0.0  10.0   0.0   120.0     66.0   \n",
       "2112-05-23 14:36:23.220999998  76.8    0.0  14.4   0.0     NaN      NaN   \n",
       "2112-05-23 14:37:23.220999997  74.9    0.0  14.8   0.0     NaN      NaN   \n",
       "2112-05-23 14:38:23.220999996  75.3   41.6  14.0  94.8     NaN      NaN   \n",
       "...                             ...    ...   ...   ...     ...      ...   \n",
       "2112-05-24 17:34:23.220998380   0.0    0.0   0.0   0.0     NaN      NaN   \n",
       "2112-05-24 17:35:23.220998379   0.0    0.0   0.0   0.0     NaN      NaN   \n",
       "2112-05-24 17:36:23.220998378   0.0    0.0   0.0   0.0     NaN      NaN   \n",
       "2112-05-24 17:37:23.220998377   0.0    0.0   0.0   0.0     NaN      NaN   \n",
       "2112-05-24 17:38:23.220998376   0.0    0.0   0.0   0.0     NaN      NaN   \n",
       "\n",
       "                               NBPMean  \n",
       "2112-05-23 14:34:23.221000000      NaN  \n",
       "2112-05-23 14:35:23.220999999     86.0  \n",
       "2112-05-23 14:36:23.220999998      NaN  \n",
       "2112-05-23 14:37:23.220999997      NaN  \n",
       "2112-05-23 14:38:23.220999996      NaN  \n",
       "...                                ...  \n",
       "2112-05-24 17:34:23.220998380      NaN  \n",
       "2112-05-24 17:35:23.220998379      NaN  \n",
       "2112-05-24 17:36:23.220998378      NaN  \n",
       "2112-05-24 17:37:23.220998377      NaN  \n",
       "2112-05-24 17:38:23.220998376      NaN  \n",
       "\n",
       "[1625 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wfdb\n",
    "\n",
    "r = wfdb.rdrecord('/Users/michaelscott/bd4h/project/streaming-env/time-series-kafka-demo/data/waveform/physionet.org/files/mimic3wdb-matched/1.0/p00/p000194/p000194-2112-05-23-14-34n')\n",
    "r.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelscott/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import get_global_config, get_waveform_path, get_base_time, get_ending_time, plot_waveform,build_spark_session\n",
    "\n",
    "from pyspark.sql.functions import datediff, to_date, max as max_, lit, col, collect_list, row_number, concat_ws, format_number, concat, monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.pandas.frame import DataFrame\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import wfdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 18:46:14 WARN Utils: Your hostname, Michaels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.118 instead (on interface en0)\n",
      "23/04/09 18:46:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/michaelscott/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/michaelscott/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/michaelscott/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-87436ffe-a77a-4dfb-a9c6-3624454ead13;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in spark-list\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 7017ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from spark-list in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   3   |   3   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-87436ffe-a77a-4dfb-a9c6-3624454ead13\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/17ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 18:46:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "cfg = get_global_config()\n",
    "spark = build_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_into_pyspark(file, schema=None, cache=False):\n",
    "    if not schema:\n",
    "        df=spark.read.csv(f'file:/{cfg[\"MIMICPATH\"]}/{file}.csv.gz', header=True, inferSchema=True)\n",
    "    else:\n",
    "        df=spark.read.csv(f'file:/{cfg[\"MIMICPATH\"]}/{file}.csv.gz', header=True, schema=schema)\n",
    "    if cache:\n",
    "        df.cache()\n",
    "    df.createOrReplaceTempView(file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "PATIENTS = read_into_pyspark('PATIENTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ROW_ID: int, SUBJECT_ID: int, GENDER: string, DOB: timestamp, DOD: timestamp, DOD_HOSP: timestamp, DOD_SSN: timestamp, EXPIRE_FLAG: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|SUBJECT_ID|\n",
      "+----------+\n",
      "|      2659|\n",
      "|      3794|\n",
      "|      4900|\n",
      "|      4935|\n",
      "|      7253|\n",
      "|     12799|\n",
      "|     13840|\n",
      "|     15619|\n",
      "|     15727|\n",
      "|     17753|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find matched patients\n",
    "# The file \"/Users/michaelscott/bd4h/project/data/waveform/physionet.org/files/mimic3wdb-matched/1.0/RECORDS\"\n",
    "# contains a list of all the records in the waveform database.\n",
    "# Each line is \"p00/p000085/\"\n",
    "# Now, extract all the patient IDs that are present in this file\n",
    "# and that will be used to filter all the other tables.\n",
    "\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "# Read the RECORDS file as a text file and split each line into patient IDs\n",
    "# convert string to int\n",
    "patient_ids = spark.read.text(f\"file://{cfg['WAVEFPATH']}/RECORDS\") \\\n",
    "                    .select(substring(\"value\", 6, 6).alias(\"SUBJECT_ID\")) \\\n",
    "                    .selectExpr(\"CAST(SUBJECT_ID AS INT) AS SUBJECT_ID\")\\\n",
    "                    .distinct()\n",
    "\n",
    "patient_ids.createOrReplaceTempView('matched_patients')\n",
    "# Show the first 10 patient IDs in the DataFrame\n",
    "patient_ids.show(10)\n",
    "\n",
    "# Use the patient IDs to filter another PySpark table\n",
    "# other_table_filtered = other_table.join(patient_ids, on=\"patient_id\", how=\"inner\")\n",
    "\n",
    "# Get all patient_id from matched_patients as a list\n",
    "patientid = patient_ids.select('SUBJECT_ID').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_age_df = spark.sql(\"\"\"SELECT matched_patients.SUBJECT_ID, date_format(DOB, 'yyyy-MM-dd') dob FROM patients \n",
    "INNER JOIN matched_patients \n",
    "ON matched_patients.subject_id = patients.subject_id\"\"\")\n",
    "\n",
    "# Write df to CSV\n",
    "patients_age_df.coalesce(1).write.csv(f'file://{cfg[\"EXPLOREPATH\"]}/patients_age', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "\n",
    "# Convert Pandas DataFrame to SQL insert statements\n",
    "insert_sql = pdf.to_sql(table_name, conn, if_exists=\"append\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd4hproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
